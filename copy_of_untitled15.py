# -*- coding: utf-8 -*-
"""Copy of Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Grfy-CmW67kTahCcGm9wFkqsj-pv3u6
"""

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv("german_credit.csv")
y = df["target"]  # adjust to dataset
X = df.drop(columns=["target"])
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer

cat_cols = X.select_dtypes(include=["object", "category"]).columns
num_cols = X.select_dtypes(include=["int64","float64"]).columns

preprocess = ColumnTransformer(
    transformers=[
        ("num", Pipeline([("imp", SimpleImputer(strategy="median"))]), num_cols),
        ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")),
                          ("ohe", OneHotEncoder(handle_unknown="ignore"))]), cat_cols),
    ]
)
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.pipeline import Pipeline

xgb = XGBClassifier(
    n_estimators=500, max_depth=4, learning_rate=0.05,
    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,
    random_state=42, n_jobs=-1
)

model = Pipeline([("prep", preprocess), ("clf", xgb)])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
auc_cv = cross_val_score(model, X, y, cv=cv, scoring="roc_auc")
print("CV AUC:", auc_cv.mean(), auc_cv.std())

model.fit(X_train, y_train)
pred_proba = model.predict_proba(X_test)[:,1]
pred = (pred_proba >= 0.5).astype(int)
print("Test AUC:", roc_auc_score(y_test, pred_proba))
print("F1:", f1_score(y_test, pred))
print("Precision:", precision_score(y_test, pred))
print("Recall:", recall_score(y_test, pred))
import shap
# Fit a model on preprocessed arrays for SHAP clarity
model.fit(X_train, y_train)
# Get preprocessed matrices
from sklearn import set_config; set_config(transform_output="pandas")
X_train_p = model.named_steps["prep"].fit_transform(X_train)
X_test_p = model.named_steps["prep"].transform(X_test)
xgb_fitted = model.named_steps["clf"]

explainer = shap.TreeExplainer(xgb_fitted)
shap_values = explainer.shap_values(X_test_p)

shap.summary_plot(shap_values, X_test_p)           # global importance
shap.summary_plot(shap_values, X_test_p, plot_type="bar")
shap.dependence_plot("Duration_in_months", shap_values, X_test_p)  # adjust feature name
i = 0  # index of a test instance
shap.plots.waterfall(shap.Explanation(values=shap_values[i],
                                      base_values=explainer.expected_value,
                                      data=X_test_p.iloc[i],
                                      feature_names=X_test_p.columns))

# LIME (optional)
from lime.lime_tabular import LimeTabularExplainer
expl = LimeTabularExplainer(
    X_train_p.values, feature_names=X_train_p.columns, class_names=['no_default','default'],
    mode='classification'
)
explanation = expl.explain_instance(
    X_test_p.iloc[i].values, xgb_fitted.predict_proba, num_features=10
)
explanation.show_in_notebook()
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

lr = Pipeline([("prep", preprocess), ("clf", LogisticRegression(max_iter=1000))])
rf = Pipeline([("prep", preprocess), ("clf", RandomForestClassifier(n_estimators=300, random_state=42))])

for name, m in [("LogReg", lr), ("RandomForest", rf), ("XGBoost", model)]:
    m.fit(X_train, y_train)
    p = m.predict_proba(X_test)[:,1]
    yhat = (p >= 0.5).astype(int)
    print(name, "AUC:", roc_auc_score(y_test, p), "F1:", f1_score(y_test, yhat))

